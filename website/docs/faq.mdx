import Collapse from '@site/src/components/Collapse';

# ⁉️ Frequently Asked Questions

<Collapse title="How much VRAM a LLM model consumes?">

By default, Tabby operates in int8 mode with CUDA, requiring approximately 8GB of VRAM for CodeLlama-7B.

For ROCm the actual limits are currently largely untested, but the same CodeLlama-7B seems to use 8GB of VRAM as well on a AMD Radeon™ RX 7900 XTX according to the ROCm monitoring tools.

</Collapse>

<Collapse title="What GPUs are required for reduced-precision inference (e.g int8)?">

* int8: Compute Capability >= 7.0 or Compute Capability 6.1
* float16: Compute Capability >= 7.0
* bfloat16: Compute Capability >= 8.0

To determine the mapping between the GPU card type and its compute capability, please visit [this page](https://developer.nvidia.com/cuda-gpus)

</Collapse>

<Collapse title="How to utilize multiple NVIDIA GPUs?">

Tabby only supports the use of a single GPU. To utilize multiple GPUs, you can initiate multiple Tabby instances and set CUDA_VISIBLE_DEVICES (for cuda) or HIP_VISIBLE_DEVICES (for rocm) accordingly.

</Collapse>

<Collapse title="My AMD device isn't supported by ROCm">

You can use the HSA_OVERRIDE_GFX_VERSION variable if there is a similar GPU that is supported by ROCm you can set it to that.

For example for RDNA2 you can set it to 10.3.0 and to 11.0.0 for RDNA3.

</Collapse>

<Collapse title="How can I convert my own model for use with Tabby?">

Since version 0.5.0, Tabby's inference now operates entirely on llama.cpp, allowing the use of any GGUF-compatible model format with Tabby. To enhance accessibility, we have curated models that we benchmarked, available at [registry-tabby](https://github.com/TabbyML/registry-tabby)

Users are free to fork the repository to create their own registry. If a user's registry is located at `https://github.com/USERNAME/registry-tabby`, the model ID will be `USERNAME/model`.

For details on the registry format, please refer to [models.json](https://github.com/TabbyML/registry-tabby/blob/main/models.json)

</Collapse>

<Collapse title="Can I use local model with Tabby?">

Tabby also supports loading models from a local directory that follow our specifications as outlined in [MODEL_SPEC.md](https://github.com/TabbyML/tabby/blob/main/MODEL_SPEC.md).

</Collapse>

<Collapse title="The computer doesn't have internet access.">
The plan is: we'll create a Docker image on a computer that's connected to the internet. Then, we'll copy the image to the computer that doesn't have internet access and run the Docker image there. Here are the steps:

1. On a computer with internet access, create a new **Dockerfile**.
```
FROM tabbyml/tabby

RUN /opt/tabby/bin/tabby-cpu download --model TabbyML/StarCoder-1B
```
2. Build the Docker image which containing the model
```bash
docker build -t tabby-offline .
```
3. Save the Docker image to a tar file:
```bash
docker save -o tabby-offline.tar tabby-offline
```
4. Copy the `tabby-offline.tar` file to the computer without internet access.
5. On the offline computer, load the Docker image from the tar file:
```bash
docker load -i tabby-offline.tar
```
6. Run the Tabby container
```bash
docker run -it \
  --gpus all -p 8080:8080 -v $HOME/.tabby:/data \
  tabby-offline \
  serve --model TabbyML/StarCoder-1B --device cuda
```


</Collapse>
