import CodeBlock from '@theme/CodeBlock';

# ⁉️ Frequently Asked Questions

<details>
  <summary>How much VRAM a LLM model consumes?</summary>
  <div>By default, Tabby operates in int8 mode with CUDA, requiring approximately 8GB of VRAM for CodeLlama-7B.</div>
</details>

<details>
  <summary>What GPUs are required for reduced-precision inference (e.g int8)?</summary>
  <div>
    <ul>
      <li>int8: Compute Capability >= 7.0 or Compute Capability 6.1</li>
      <li>float16: Compute Capability >= 7.0</li>
      <li>bfloat16: Compute Capability >= 8.0</li>
    </ul>
    <p>
      To determine the mapping between the GPU card type and its compute capability, please visit <a href="https://developer.nvidia.com/cuda-gpus">this page</a>
    </p>
  </div>
</details>

<details>
  <summary>How to utilize multiple NVIDIA GPUs?</summary>
  <div>
    <p>Tabby only supports the use of a single GPU. To utilize multiple GPUs, you can initiate multiple Tabby instances and set CUDA_VISIBLE_DEVICES accordingly.</p>
  </div>
</details>

<details>
  <summary>How can I convert my own model for use with Tabby?</summary>
  <div>
    <p>Since version 0.5.0, Tabby's inference now operates entirely on llama.cpp, allowing the use of any GGUF-compatible model format with Tabby. To enhance accessibility, we have curated models that we benchmarked, available at <a href="https://github.com/TabbyML/registry-tabby">registry-tabby</a>.</p>
    <p>Users are free to fork the repository to create their own registry. If a user's registry is located at <code>https://github.com/USERNAME/registry-tabby</code>, the model ID will be <code>USERNAME/model</code>.</p>
    <p>For details on the registry format, please refer to <a href="https://github.com/TabbyML/registry-tabby/blob/main/models.json">models.json</a></p>
  </div>
</details>

<details>
  <summary>How can I run tabby offline?</summary>
  <div>
    <ul>   
      <li>
        First download the models and docker image on an internet connected machine:
        <CodeBlock language="bash">
        {`# Download the docker container and models
docker run --rm --name tabby -it -p 8080:8080 -v $HOME/.tabby:/data tabbyml/tabby download --model TabbyML/StarCoder-1B      
# Compress the models directory
tar zcvf models.tgz $HOME/.tabby
# Save the docker image
docker save -o tabbyml.tar tabbyml/tabby`}
        </CodeBlock>
      </li>
      <li>Then transfer the files <code>models.tgz</code> and <code>tabbyml.tar</code> to the offline system.</li>
      <li>
        Prepare tabby on the offline system:
        <CodeBlock language="bash">
        {`# Load the docker image
docker load -i tabbyml.tar
# Extract the models directory
tar -zxvf models.tgz -C $HOME
# Start tabby
docker run --name tabby -it -p 8080:8080 -v $HOME/.tabby:/data tabbyml/tabby serve --model TabbyML/StarCoder-1B`}
        </CodeBlock>
      </li>
      <li>
        Launch tabby as described in the <a href="https://tabby.tabbyml.com/docs/installation/docker">Docker</a> installation instructions.
      </li>
    </ul>
  </div>
</details>
