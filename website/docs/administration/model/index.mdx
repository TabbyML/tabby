import ModelCardUrl from "./model_card.png";

# Model Configuration

Tabby provide two kinds of models connection:

1. Agent Model
2. Completion Model

You can configure how Tabby connects with LLM models by editing the `~/.tabby/config.toml` file.
Each of them can be configured individually.

- **Agent Model**: The Agent model is adept at producing conversational replies and is broadly compatible with OpenAI's standards.
- **Completion Model**: The Completion model is designed to provide suggestions for code completion, focusing mainly on the Fill-in-the-Middle (FIM) prompting style.

See the following examples of how to configure the model settings in the `~/.tabby/config.toml` file:

```toml title="~/.tabby/config.toml"
[upstreams.openai]
kind = "openai"
api_endpoint = "https://api.openai.com/v1"
api_key = "API_KEY"
models = []                 # auto-fetch by /models api if possible
models_cache_ttl = "10m"

[upstreams.others]
kind = "openai"
api_endpoint = "https://mistral.ai/v1"
api_key = "API_KEY"
models = ["codellama:7b"]   # explicit, no fetch

[agent]
upstream = "openai"

[completion]
upstream = "others"
```

## Verifying Model Connection Status

To check whether your configured models are properly connected, navigate to the **Information > System** page and check individual model cards.

<img src={ModelCardUrl} alt="Model Card" />
