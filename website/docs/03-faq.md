# Frequently Asked Questions

<details>
    <summary>How much VRAM a LLM model consumes?</summary>

    By default, Tabby operates in int8 mode with CUDA, requiring approximately 8GB of VRAM for CodeLlama-7B.
</details>
