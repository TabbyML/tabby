ARG UBUNTU_VERSION=18.04
FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu${UBUNTU_VERSION} as builder

ENV TENSORRT_MAJOR_VERSION=6
ENV TENSORRT_VERSION=${TENSORRT_MAJOR_VERSION}.0.1

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        apt-transport-https \
        build-essential \
        ca-certificates \
        libboost-program-options-dev \
        libboost-python-dev \
        python-pip \
        gnupg2 \
        curl \
        libnvinfer${TENSORRT_MAJOR_VERSION}=${TENSORRT_VERSION}-1+cuda10.0 \
        libnvinfer-dev=${TENSORRT_VERSION}-1+cuda10.0 \
        wget && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /root

RUN wget https://cmake.org/files/v3.12/cmake-3.12.2-Linux-x86_64.tar.gz
RUN tar xf cmake-3.12.2-Linux-x86_64.tar.gz && \
    rm cmake-3.12.2-Linux-x86_64.tar.gz
ENV PATH=$PATH:/root/cmake-3.12.2-Linux-x86_64/bin

ENV MKL_VERSION=2019
ENV MKL_UPDATE=5
ENV MKL_BUILD=075
RUN wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS-2019.PUB && \
    apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-*.PUB && \
    rm GPG-PUB-KEY-INTEL-SW-PRODUCTS-*.PUB && \
    echo "deb https://apt.repos.intel.com/mkl all main" > /etc/apt/sources.list.d/intel-mkl.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
        intel-mkl-64bit-$MKL_VERSION.$MKL_UPDATE.$MKL_BUILD && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

ENV MKLDNN_ROOT=/root/mkl-dnn
ENV MKLDNN_VERSION=0.21
RUN wget https://github.com/intel/mkl-dnn/archive/v$MKLDNN_VERSION.tar.gz && \
    tar xf v$MKLDNN_VERSION.tar.gz && rm v$MKLDNN_VERSION.tar.gz && \
    cd mkl-dnn-* && \
    mkdir build && cd build && \
    cmake -DCMAKE_INSTALL_PREFIX=${MKLDNN_ROOT} -DARCH_OPT_FLAGS="" \
          -DMKLROOT=/opt/intel/mkl -DMKLDNN_USE_MKL=FULL:STATIC -DMKLDNN_THREADING=OMP:INTEL \
          -DWITH_TEST=OFF -DWITH_EXAMPLE=OFF .. && \
    make -j4 && make install && \
    cd ../.. && rm -r mkl-dnn-*

ENV CUB_VERSION=1.8.0
ENV CUB_ROOT=/root/cub
RUN wget https://github.com/NVlabs/cub/archive/v${CUB_VERSION}.tar.gz && \
    tar xf v${CUB_VERSION}.tar.gz && \
    mv cub-${CUB_VERSION} ${CUB_ROOT} && \
    rm v${CUB_VERSION}.tar.gz

WORKDIR /root/ctranslate2-dev

COPY cli cli
COPY include include
COPY src src
COPY tests tests
COPY CMakeLists.txt .

ARG CXX_FLAGS
ENV CXX_FLAGS=${CXX_FLAGS}
ARG CUDA_NVCC_FLAGS
ENV CUDA_NVCC_FLAGS=${CUDA_NVCC_FLAGS:-"-Xfatbin -compress-all"}
ARG CUDA_ARCH_LIST
ENV CUDA_ARCH_LIST=${CUDA_ARCH_LIST:-"Common"}

RUN mkdir build && \
    cd build && \
    cmake -DCMAKE_INSTALL_PREFIX=/root/ctranslate2 \
          -DCMAKE_PREFIX_PATH="${CUB_ROOT};${MKLDNN_ROOT}" \
          -DWITH_CUDA=ON -DWITH_MKLDNN=ON \
          -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS="${CXX_FLAGS}" \
          -DCUDA_NVCC_FLAGS="${CUDA_NVCC_FLAGS}" -DCUDA_ARCH_LIST="${CUDA_ARCH_LIST}" .. && \
    VERBOSE=1 make -j4 && \
    make install

COPY python python

WORKDIR /root/ctranslate2-dev/python
RUN pip --no-cache-dir install setuptools wheel
RUN CTRANSLATE2_ROOT=/root/ctranslate2 python setup.py bdist_wheel

WORKDIR /root
RUN cp /opt/intel/lib/intel64/libiomp5.so /root/ctranslate2/lib && \
    cp -P /root/mkl-dnn/lib/libmkldnn.so* /root/ctranslate2/lib && \
    cp -P /usr/lib/x86_64-linux-gnu/libboost_python*.so* /root/ctranslate2/lib && \
    cp -P /usr/lib/x86_64-linux-gnu/libcudnn.so* /root/ctranslate2/lib && \
    cp -P /usr/lib/x86_64-linux-gnu/libnvinfer.so* /root/ctranslate2/lib && \
    cp /root/ctranslate2-dev/python/dist/*whl /root/ctranslate2

FROM nvidia/cuda:10.0-base-ubuntu${UBUNTU_VERSION}

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        cuda-cublas-$CUDA_PKG_VERSION \
        python-pip && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

COPY --from=builder /root/ctranslate2 /opt/ctranslate2
RUN pip --no-cache-dir install /opt/ctranslate2/*.whl

WORKDIR /opt

ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/ctranslate2/lib

ENTRYPOINT ["/opt/ctranslate2/bin/translate"]
