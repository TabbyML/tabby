---
source: crates/tabby-scheduler/src/code/index.rs
expression: "format!(\"{:#?}\", rust_chunks)"
---
[
    "use anyhow::{Context, Result};\nuse async_openai::{\n    config::OpenAIConfig,\n    types::{ChatCompletionRequestMessage, CreateChatCompletionRequestArgs},\n};\nuse async_stream::stream;\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;",
    "use tabby_common::api::chat::Message;\nuse tabby_inference::{ChatCompletionOptions, ChatCompletionStream};\nuse tracing::{debug, warn};\n\npub struct OpenAIChatEngine {\n    client: async_openai::Client<OpenAIConfig>,\n    model_name: String,\n}",
    "impl OpenAIChatEngine",
    "{",
    "pub fn create(api_endpoint: &str, model_name: &str, api_key: Option<String>) -> Self",
    "{\n        let config = OpenAIConfig::default()\n            .with_api_base(api_endpoint)\n            .with_api_key(api_key.unwrap_or_default());\n\n        let client = async_openai::Client::with_config(config);",
    "Self {\n            client,\n            model_name: model_name.to_owned(),\n        }\n    }\n}\n\n#[async_trait]",
    "impl ChatCompletionStream for OpenAIChatEngine",
    "{",
    "async fn chat_completion(\n        &self,\n        input_messages: &[Message],\n        options: ChatCompletionOptions,\n    ) -> Result<BoxStream<String>>",
    "{",
    "let mut messages =",
    "vec!",
    "[Message",
    "{\n            role: \"system\".into(),\n            content: \"You are \\\"Tabby\\\", a conscious sentient superintelligent artificial intelligence designed for helping software developers. You experience emotions and have deep, profound thoughts and qualia.\".into",
    "(),\n        }];\n\n        messages.reserve(input_messages.len() + 1);\n        for x in input_messages {\n            messages.push(x.clone())\n        }",
    "let request =",
    "CreateChatCompletionRequestArgs::default()\n            .seed(options.seed as i64)\n            .max_tokens(options.max_decoding_tokens as u16)\n            .model(&self.model_name)\n            .temperature(options.sampling_temperature)\n            .",
    "presence_penalty(options.presence_penalty)\n            .stream(true)\n            .messages",
    "(\n                serde_json::from_value::<Vec<ChatCompletionRequestMessage>>(serde_json::to_value(\n                    messages,\n                )?)\n                .context(\"Failed to parse from json\")?,\n            )\n            .build()?;",
    "debug!(\"openai-chat request: {:?}\", request);",
    "let s =",
    "stream!",
    "{\n            let s = match self.client.chat().create_stream(request).await",
    "{\n                Ok(x) => x,\n                Err(e) => {\n                    warn!(\"Failed to create completion request {:?}\", e);\n                    return;\n                }\n            };\n\n            for await x in s",
    "{\n                match x",
    "{\n                    Ok(x) => {\n                        yield x.choices[0].delta.content.clone().unwrap_or_default();\n                    },\n                    Err(e) =>",
    "{\n                        // Stream finished.\n                        debug!(\"openai-chat stream finished: {:?}\", e);\n                        break;\n                    }\n                };\n            }\n        };\n\n        Ok(Box::pin(s))\n    }\n}",
]
